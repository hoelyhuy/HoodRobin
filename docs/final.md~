---
layout: default
title: Final Report
---
## Video
## Project Summary
In our Minecraft-AI project, the agent tries to recognize different animals that appear in his view such as Pig, Rabbit, Ozelot, Sheep, Cow and then attempts to attack the Pig with a bow and arraows. Since we are interested in computer vision, we will attempt to use a real-time object detection algorithm called YOLO (You Only Look Once) to detect to detect which animals are in the agents view and where are the animals in the agent's view. Object detection can be very slow if we don't have the right approach. For instance, given an image and the objective to detect the objects in the image, one naive solution is to predict all the possible bounding box that we can draw in the image, which is very inefficient. Inorder to detect objects in real-time, we need a more sophisticated algorithm that run fast but still gives correct predictions with high accuracy.
## Approaches
Firstly, we write a program to generate the dataset, which is a set of many image captures of the agent's view. To do this, we start with designing the world, which is a piece of farmland surrounded by fences on four sides. On that piece of land, we spawn some animals (such as pigs, cows, sheeps, etc) at random locations. As the program starts, the agents takes a screenshot of his current view every 3 seconds. Malmo can give us the latest frame in the form of array of pixels. Given the array of pixels, we use image processing tools such as Pillow and opencv to convert the array of pixels into an image file and store it to our computer. Before, we trained our neural network on a dataset of about 400 screenshots and the agent performs quite well. For final submission, we decided to train our neural network on about 1300 images to improve the accuracy of object detection.
<br />
<br />
After collecting the screenshots, we use an image labeling tool, which is called labelImg to create annotation files for the screenshots. An annotation file is an additional file that tells more information about the image; for this project, the additional information is which objects are in the image and where are they in the image. labelImg allows us to draw bounding box around an object and label that object, then generate an annotation file that is need for training. 
<br />
<br />
We will be using a real-time object detection algorithm called YOLO (You Only Look Once) to detect the animals in the agent's view. How the object detection algorithm works is that we apply a single neural network to the full image. This network divdes the image into regions and predict bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities. The model has several advantages over classifier-based systems. It looks at the whole image at test time so its predictions are informed by global context in the image. It also makes predictions with a single network evaluation unlike systems like R-CNN which require thousands for a single image. This makes it extremely fast, more that 1000 times faster than R-CNN and 100 times faster than Fast R-CNN. [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/)
<br />
<br />
Darknet is an open source neural network framework written in C and CUDA. It supports both CPU and GPU computation. To carry out training and object detection, we use darkflow, which is a translation of darknet to tensorflow. Once we are done with generating the screenshots and the annotation file, we store the screenshots and the annotation files in darkflow workspace. In addition to our own dataset, we also need YOLOv2 configuration file (with some small adjustments) and YOLOv2 weights to train. For training we use Google Colab, which supports Python notebook and many other useful machine learning tools such as opencv, tensorflow, numpy, plus a free GPU that helps to improve training speed. After a number of interations of training, we look for a point where the loss converges and stop our training there.
<br />
<br />
Once we are done with training, we can use our network to detect the objects in Minecraft while the game is running. Darkflow returns image detection results in a form of json array of json object. Each json object contains the information about the label of the object, its bounding box which is specified by the coordinate of the top left corner and the coordinate of the bottom right corner, confidence level of prediction. We extract this information to find the center of the bounding box around the target object that we are most confident about and then move the agent's aim towards that pointand then shoot an arrow at the target.
## Evaluation
Before incorporating our network into the game we need to evaluate its performance. In order to evaluate the performance of our model we generate a test set of 300 screenshots, run darkflow on the test set and collect the output. Two types of evaluation will be carried out to evaluate the performance of the model which are qualitative evaluation and quantitative evaluation. For setting up the evaluation, we generate 300 test image, run darkflow on the test data once with the old weights that we had for previous submisstion and once with 
For quantitative evaluation, we measure the accuracy of our object detection system as the number of correct detections over the total number of objects, which are counted manually. We consider an animal one object when about more than 30% of the animal's body can be seen. For example, if the back legs of a pig is visible, we count that pig as one object. If only the head of an ozelot is visible, we still count that ozelot one object. However if only the tail of an ozelot is visible then we don't count that ozelot as one object. Also if only two horns of a cow is visible, we don't count that cow as an object. Then we compare the accuracy of the image detector we use for final submission versus the object detector we had before to verify our hypothesis that whether increasing dataset size would improve the image detector's performance.
<br />
<br />
For qualitative evaluation, we measure how well the boxes are bounded around the objects. The result for qualitative evaluation will be a percentage. It is the number of good bounding box over the total number of bounding box (regardless of what the labels are). A good bounding box bounds nicely around the visible part of the object, as some part of the object can be blocked by other objects. A bad bounding box only covers partial of the visible part of the object. Double bounding box, which is two boxes bounding the same object, is also considered one bad bounding box.
## References
[Image Processing](https://github.com/jennyzeng/Minecraft-AI)
<br />
[Image Labeling software](https://github.com/tzutalin/labelImg)
<br />
[YOLO: Real-Time Object Detection (Tensorflow)](https://github.com/thtrieu/darkflow)
<br />
[YOLO: Real-Time Object Detection (C)](https://pjreddie.com/darknet/yolo/)
<br />
[Project inspiration](https://www.youtube.com/watch?v=4eIBisqx9_g&t=444s)
